{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ed3e73-9757-4955-875d-8b1691225a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np                              # Used for numerical operations and array handling\n",
    "\n",
    "from tensorflow.keras.models import Sequential   # Allows building a neural network model layer-by-layer\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda  # Required layers for CBOW model\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer     # Converts text into sequences of integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Helps to pad sequences to equal length\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical             # Converts target values to one-hot encoding\n",
    "\n",
    "import tensorflow.keras.backend as K                          # Allows defining custom operations (used for averaging embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab24c8d-f21e-4ef1-9366-488804b04f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is to be imported in exam, we can write:\n",
    "# Example: loading a text dataset from a file\n",
    "# with open(\"dataset.txt\", \"r\") as f:\n",
    "#     text = f.read()\n",
    "#\n",
    "# Or from CSV:\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv(\"file.csv\")\n",
    "# text = \" \".join(data['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c9418e-f0e3-44a7-a154-ae167439f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no file is loaded, fallback sample sentences:\n",
    "#text = [\n",
    "# \"Machine learning models can learn word embeddings\",\n",
    "#  \"Continuous Bag of Words is one Word2Vec model\",\n",
    "#   \"Neural networks are powerful tools for NLP tasks\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927b969f-dc24-4597-a2a5-b5e64a2671d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to numerical indices\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the given text data \n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "# Vocabulary size = total unique words + 1 (indexing starts from 1)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# Convert the text into sequences of integers based on the learned vocabulary\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "\n",
    "# Window size: Number of context words to take from each side of the target word\n",
    "window_size = 2\n",
    "\n",
    "# Dimension of the embedding vector for each word\n",
    "embedding_dim = 10\n",
    "\n",
    "\n",
    "# Lists to store context words (inputs) and target word (output)\n",
    "contexts = []\n",
    "targets = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ed8fd5-ba00-4e92-a9b8-7b0866c2fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training Data for CBOW Model\n",
    "for sentence in sequences:                          # Go through each sentence in the dataset\n",
    "    for i, word in enumerate(sentence):             # For each word in the sentence\n",
    "        # Define the context window boundaries around the target word\n",
    "        start = max(0, i - window_size)\n",
    "        end = i + window_size + 1\n",
    "\n",
    "        # Collect context words (exclude the target word itself)\n",
    "        context_words = [sentence[j] for j in range(start, end)\n",
    "                         if j != i and j < len(sentence)]\n",
    "\n",
    "        # Pad context if fewer words (for words near the beginning or end of sentence)\n",
    "        if len(context_words) < window_size * 2:\n",
    "            context_words = [0]*(window_size*2 - len(context_words)) + context_words\n",
    "        \n",
    "        # Store context and corresponding target word\n",
    "        contexts.append(context_words)\n",
    "        targets.append(word)\n",
    "\n",
    "# Convert lists to NumPy arrays for model training\n",
    "X = np.array(contexts)\n",
    "\n",
    "# Convert target words to one-hot encoded format\n",
    "y = to_categorical(targets, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8436a1f-4549-4b4c-910d-442270e1c998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 4, 10)             240       \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 504\n",
      "Trainable params: 504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 4ms/step - loss: 3.1766\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 3.1736\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 3.1712\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 3.1688\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.1664\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 3.1642\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 3.1619\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 3.1595\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 3.1571\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 3.1548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1765ebace20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the CBOW Model\n",
    "model = Sequential([\n",
    "    # Embedding layer converts word indices into dense vector representations\n",
    "    Embedding(vocab_size, embedding_dim, input_length=window_size * 2),\n",
    "\n",
    "    # Lambda layer takes the average of all context word embeddings (CBOW concept)\n",
    "    Lambda(lambda x: K.mean(x, axis=1)),\n",
    "\n",
    "    # Output layer with softmax to predict the target word from vocabulary\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Display model structure\n",
    "model.summary()\n",
    "\n",
    "# Train the model with training data\n",
    "model.fit(X, y, epochs=10, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a15952-054f-405b-ac5e-cbe0d12ab073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and embeddings saved to vectors_simple.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned word embeddings (weights of the embedding layer)\n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Open a file to save the word vectors in a readable format\n",
    "with open(\"vectors_simple.txt\", \"w\") as f:\n",
    "    # Write vocabulary size and embedding dimension in the first line (standard format)\n",
    "    f.write(f\"{vocab_size} {embedding_dim}\\n\")\n",
    "\n",
    "    # For each word in the vocabulary, write the word followed by its vector values\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        vector = weights[i]                          # Get embedding vector of the word\n",
    "        vector_str = ' '.join(map(str, vector))      # Convert numeric values to string\n",
    "        f.write(f\"{word} {vector_str}\\n\")            # Write to file in \"word val1 val2 ...\" format\n",
    "\n",
    "# Final message to indicate completion\n",
    "print(\"Training complete and embeddings saved to vectors_simple.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755d63c-b3cb-4ecc-9c1f-96eede060856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
